import os
import sys
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from diffusers import (
    ControlNetModel,
    AutoencoderKL,
    UNet2DConditionModel,
    DDPMScheduler,
)
from PIL import Image
from tqdm.auto import tqdm
import warnings

# ================= ğŸš€ 5090 æœ€ç»ˆæˆ˜åœºé…ç½® =================
# 1. è·¯å¾„é…ç½® (ç»å¯¹æ­£ç¡®ç‰ˆ)
NUSC_ROOT = "./nuScenes"            # åŸå§‹ç…§ç‰‡åœ¨è¿™é‡Œ
DATA_ROOT = "./training_data"       # åœ°å›¾(hdmaps)åœ¨è¿™é‡Œ
OUTPUT_DIR = "./models/unimvm_video_model"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# 2. 5090 æ€§èƒ½å…¨å¼€
# è¿™ä¸€æ­¥æ¨¡å‹å¾ˆå¤§ï¼ŒBatch Size è®¾ä¸º 4 æˆ– 8 (å¦‚æœæ˜¾å­˜ä¸å¤Ÿä¼šè‡ªåŠ¨æŠ¥é”™ï¼Œ5090 åº”è¯¥èƒ½æŠ— 8)
BATCH_SIZE = 8
MAX_TRAIN_STEPS = 5000   # è®­ç»ƒ 5000 æ­¥ (çº¦ 20-40 åˆ†é’Ÿ)
LEARNING_RATE = 1e-5
IMG_SIZE = [256, 448]    # [é«˜, å®½] è®ºæ–‡æ ‡å‡†åˆ†è¾¨ç‡

# 3. åŸºç¡€æ¨¡å‹
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
MODEL_NAME = "runwayml/stable-diffusion-v1-5" 

warnings.filterwarnings("ignore")
# =======================================================

class UniMVMDataset(Dataset):
    def __init__(self, processed_root, nusc_root):
        self.map_dir = os.path.join(processed_root, "hdmaps")
        print(f"ğŸš€ [1/3] æ­£åœ¨åŠ è½½ nuScenes... (Root: {nusc_root})")
        
        from nuscenes.nuscenes import NuScenes
        try:
            self.nusc = NuScenes(version='v1.0-mini', dataroot=nusc_root, verbose=False)
        except Exception as e:
            print(f"âŒ nuScenes æŠ¥é”™: {e}")
            sys.exit(1)

        self.data_pairs = []
        # å¯»æ‰¾å¯¹åº”çš„åœ°å›¾æ–‡ä»¶
        if not os.path.exists(self.map_dir):
             print(f"âŒ æ‰¾ä¸åˆ°åœ°å›¾æ–‡ä»¶å¤¹: {self.map_dir}")
             sys.exit(1)

        map_files = [f for f in os.listdir(self.map_dir) if f.endswith('.png')]
        
        print(f"ğŸ” [2/3] æ­£åœ¨åŒ¹é…æ•°æ® (åœ°å›¾ -> çœŸå®ç…§ç‰‡)...")
        for f in tqdm(map_files):
            sample_token = f.split('.')[0]
            try:
                sample = self.nusc.get('sample', sample_token)
                cam_token = sample['data']['CAM_FRONT']
                cam_data = self.nusc.get('sample_data', cam_token)
                
                # çœŸå®ç…§ç‰‡ (Target)
                cam_path = os.path.join(nusc_root, cam_data['filename'])
                # åœ°å›¾ (Condition)
                map_path = os.path.join(self.map_dir, f)
                
                if os.path.exists(cam_path) and os.path.exists(map_path):
                    self.data_pairs.append((map_path, cam_path))
            except:
                continue
                
        print(f"âœ… [3/3] é…å¯¹æˆåŠŸ! æœ‰æ•ˆè®­ç»ƒæ ·æœ¬: {len(self.data_pairs)} å¯¹")

        self.transform = transforms.Compose([
            transforms.Resize((IMG_SIZE[0], IMG_SIZE[1])), # 256x448
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5]),
        ])

    def __len__(self):
        return len(self.data_pairs)

    def __getitem__(self, idx):
        map_path, cam_path = self.data_pairs[idx]
        
        # æ¡ä»¶å›¾: åœ°å›¾
        control_image = Image.open(map_path).convert("RGB")
        # ç›®æ ‡å›¾: çœŸå®è¡—æ™¯
        target_image = Image.open(cam_path).convert("RGB")
        
        return {
            "pixel_values": self.transform(target_image),
            "conditioning_pixel_values": self.transform(control_image)
        }

def train():
    device = torch.device("cuda")
    torch.backends.cuda.matmul.allow_tf32 = True
    
    print(f"ğŸ”¥ Step 3 è®­ç»ƒå¯åŠ¨ | Res: {IMG_SIZE} | Steps: {MAX_TRAIN_STEPS}")
    
    # åŠ è½½ç»„ä»¶
    noise_scheduler = DDPMScheduler.from_pretrained(MODEL_NAME, subfolder="scheduler")
    vae = AutoencoderKL.from_pretrained(MODEL_NAME, subfolder="vae").to(device)
    unet = UNet2DConditionModel.from_pretrained(MODEL_NAME, subfolder="unet").to(device)
    controlnet = ControlNetModel.from_unet(unet).to(device)
    
    vae.requires_grad_(False)
    unet.requires_grad_(False)
    controlnet.train()
    
    # 8-bit ä¼˜åŒ–
    try:
        import bitsandbytes as bnb
        optimizer = bnb.optim.AdamW8bit(controlnet.parameters(), lr=LEARNING_RATE)
        print("âœ¨ å·²å¯ç”¨ bitsandbytes 8-bit ä¼˜åŒ–")
    except:
        optimizer = torch.optim.AdamW(controlnet.parameters(), lr=LEARNING_RATE)
    
    dataset = UniMVMDataset(DATA_ROOT, NUSC_ROOT)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    
    progress_bar = tqdm(range(MAX_TRAIN_STEPS), desc="Video Model Training")
    global_step = 0
    
    while global_step < MAX_TRAIN_STEPS:
        for batch in dataloader:
            with torch.cuda.amp.autocast():
                # ç¼–ç çœŸå®å›¾ç‰‡ -> Latents
                latents = vae.encode(batch["pixel_values"].to(device)).latent_dist.sample()
                latents = latents * vae.config.scaling_factor
                
                # è¯»å–åœ°å›¾æ¡ä»¶
                control_image = batch["conditioning_pixel_values"].to(device)
                
                # åŠ å™ªå£°
                noise = torch.randn_like(latents)
                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device)
                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
                
                empty_text = torch.zeros((latents.shape[0], 77, 768), device=device)
                
                # ControlNet å‰å‘
                down_res, mid_res = controlnet(
                    noisy_latents,
                    timesteps,
                    encoder_hidden_states=empty_text,
                    controlnet_cond=control_image,
                    return_dict=False,
                )
                
                # UNet å‰å‘ (æ¥å— ControlNet çš„æŒ‡å¯¼)
                model_pred = unet(
                    noisy_latents,
                    timesteps,
                    encoder_hidden_states=empty_text,
                    down_block_additional_residuals=down_res,
                    mid_block_additional_residual=mid_res,
                ).sample
                
                loss = F.mse_loss(model_pred.float(), noise.float())
            
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            progress_bar.update(1)
            progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})
            global_step += 1
            
            if global_step >= MAX_TRAIN_STEPS:
                break
        
        # æ¯ 1000 æ­¥ä¿å­˜ä¸€æ¬¡ checkpoints
        if global_step % 1000 == 0:
             controlnet.save_pretrained(OUTPUT_DIR)
             print(f"\nğŸ’¾ ä¸­é€”å­˜æ¡£å·²ä¿å­˜ (Step {global_step})")

    controlnet.save_pretrained(OUTPUT_DIR)
    print(f"\nğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰è®­ç»ƒå…¨éƒ¨å®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {OUTPUT_DIR}")

if __name__ == "__main__":
    train()