import os
import sys
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from diffusers import (
    ControlNetModel,
    AutoencoderKL,
    UNet2DConditionModel,
    DDPMScheduler,
)
from PIL import Image
from tqdm.auto import tqdm
import warnings

# ================= ğŸš€ 5090 è®­ç»ƒé…ç½® =================
# 1. è·¯å¾„ (è‡ªåŠ¨è¯»å–åˆšæ‰ç”Ÿæˆçš„ ./training_data)
DATA_ROOT = "./training_data"
OUTPUT_DIR = "./models/hdmap_controlnet"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# 2. è®­ç»ƒå‚æ•°
# 5090 æ˜¾å­˜æå¤§ï¼ŒBatch Size å¼€åˆ° 8 æ²¡é—®é¢˜ï¼Œè·‘å¾—å¿«
BATCH_SIZE = 8
# å›¾åƒå°ºå¯¸ (512x512 æ˜¯æ ‡å‡† SD åˆ†è¾¨ç‡)
IMG_SIZE = 512 
# è®­ç»ƒè½®æ•° (15è½®è¶³å¤Ÿè®©å®ƒå­¦ä¼šç”»åœ°å›¾äº†)
NUM_EPOCHS = 15 
LEARNING_RATE = 1e-5

# 3. è¿™é‡Œçš„æ¨¡å‹æ˜¯é€šç”¨çš„ SD 1.5
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
MODEL_NAME = "runwayml/stable-diffusion-v1-5" 

# å¿½ç•¥ 5090 çš„å…¼å®¹æ€§è­¦å‘Š
warnings.filterwarnings("ignore")
# ===================================================

class DriveDreamerDataset(Dataset):
    def __init__(self, root_dir):
        self.traj_dir = os.path.join(root_dir, "trajectories")
        self.map_dir = os.path.join(root_dir, "hdmaps")
        
        self.filenames = []
        # ç¡®ä¿ä¸¤ä¸ªæ–‡ä»¶å¤¹éƒ½å­˜åœ¨
        if os.path.exists(self.traj_dir) and os.path.exists(self.map_dir):
            traj_files = set(f for f in os.listdir(self.traj_dir) if f.endswith('.png'))
            map_files = set(f for f in os.listdir(self.map_dir) if f.endswith('.png'))
            # å–äº¤é›† (ç¡®ä¿æ¯ä¸€å¯¹æ•°æ®éƒ½å®Œæ•´)
            self.filenames = list(traj_files & map_files)
        
        print(f"ğŸ” æ•°æ®é›†å°±ç»ª: æ‰¾åˆ° {len(self.filenames)} ç»„è®­ç»ƒæ•°æ® (Trajectory -> Map)")
        if len(self.filenames) == 0:
            print("âŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°é…å¯¹æ•°æ®ï¼è¯·ç¡®è®¤ Step 1 å’Œ Step 2 éƒ½è·‘æˆåŠŸäº†ã€‚")
            sys.exit(1)

        self.transform = transforms.Compose([
            transforms.Resize((IMG_SIZE, IMG_SIZE)),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5]),
        ])

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):
        filename = self.filenames[idx]
        
        # è¾“å…¥æ¡ä»¶: è½¨è¿¹å›¾ (Condition)
        traj_path = os.path.join(self.traj_dir, filename)
        control_image = Image.open(traj_path).convert("RGB")
        
        # è®­ç»ƒç›®æ ‡: é«˜ç²¾åœ°å›¾ (Ground Truth)
        map_path = os.path.join(self.map_dir, filename)
        target_image = Image.open(map_path).convert("RGB")
        
        return {
            "pixel_values": self.transform(target_image),
            "conditioning_pixel_values": self.transform(control_image)
        }

def train():
    device = torch.device("cuda")
    # 5090 å¼€å¯ TF32 åŠ é€Ÿ
    torch.backends.cuda.matmul.allow_tf32 = True
    
    print(f"ğŸ”¥ RTX 5090 è®­ç»ƒå¼•æ“å¯åŠ¨ | Batch: {BATCH_SIZE} | Epochs: {NUM_EPOCHS}")
    
    # åŠ è½½æ¨¡å‹ç»„ä»¶
    noise_scheduler = DDPMScheduler.from_pretrained(MODEL_NAME, subfolder="scheduler")
    vae = AutoencoderKL.from_pretrained(MODEL_NAME, subfolder="vae").to(device)
    unet = UNet2DConditionModel.from_pretrained(MODEL_NAME, subfolder="unet").to(device)
    # åˆå§‹åŒ– ControlNet (è¿™å°±æ˜¯æˆ‘ä»¬è¦è®­ç»ƒçš„æ ¸å¿ƒ)
    controlnet = ControlNetModel.from_unet(unet).to(device)
    
    # å†»ç»“å…¶ä»–éƒ¨åˆ†ï¼Œåªè®­ç»ƒ ControlNet
    vae.requires_grad_(False)
    unet.requires_grad_(False)
    controlnet.train()
    
    # å°è¯•ä½¿ç”¨ 8-bit ä¼˜åŒ–å™¨ (çœæ˜¾å­˜ç¥å™¨)
    try:
        import bitsandbytes as bnb
        optimizer = bnb.optim.AdamW8bit(controlnet.parameters(), lr=LEARNING_RATE)
        print("âœ¨ å·²å¯ç”¨ bitsandbytes 8-bit ä¼˜åŒ–")
    except ImportError:
        print("âš ï¸ æœªæ‰¾åˆ° bitsandbytesï¼Œä½¿ç”¨åŸç”Ÿä¼˜åŒ–å™¨")
        optimizer = torch.optim.AdamW(controlnet.parameters(), lr=LEARNING_RATE)
    
    dataset = DriveDreamerDataset(DATA_ROOT)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(NUM_EPOCHS):
        progress_bar = tqdm(total=len(dataloader), desc=f"Epoch {epoch+1}/{NUM_EPOCHS}")
        total_loss = 0
        
        for batch in dataloader:
            with torch.cuda.amp.autocast(): # æ··åˆç²¾åº¦è®­ç»ƒ
                # 1. å›¾åƒç¼–ç 
                latents = vae.encode(batch["pixel_values"].to(device)).latent_dist.sample()
                latents = latents * vae.config.scaling_factor
                
                # 2. è¯»å–æ¡ä»¶ (è½¨è¿¹)
                control_image = batch["conditioning_pixel_values"].to(device)
                
                # 3. åŠ å™ªå£°
                noise = torch.randn_like(latents)
                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device)
                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
                
                empty_text = torch.zeros((latents.shape[0], 77, 768), device=device)
                
                # 4. å‰å‘ä¼ æ’­
                down_res, mid_res = controlnet(
                    noisy_latents,
                    timesteps,
                    encoder_hidden_states=empty_text,
                    controlnet_cond=control_image,
                    return_dict=False,
                )
                
                model_pred = unet(
                    noisy_latents,
                    timesteps,
                    encoder_hidden_states=empty_text,
                    down_block_additional_residuals=down_res,
                    mid_block_additional_residual=mid_res,
                ).sample
                
                loss = F.mse_loss(model_pred.float(), noise.float(), reduction="mean")
            
            # 5. åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            total_loss += loss.item()
            progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})
            progress_bar.update(1)
        
        # æ¯è½®ä¿å­˜ä¸€æ¬¡ï¼Œé˜²æ­¢æ„å¤–
        save_path = os.path.join(OUTPUT_DIR, f"checkpoint-epoch-{epoch+1}")
        controlnet.save_pretrained(save_path)
        print(f"ğŸ’¾ å­˜æ¡£å·²ä¿å­˜: {save_path}")

    # æœ€ç»ˆä¿å­˜
    controlnet.save_pretrained(OUTPUT_DIR)
    print(f"\nğŸ‰ è®­ç»ƒå¤§åŠŸå‘Šæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {OUTPUT_DIR}")

if __name__ == "__main__":
    train()