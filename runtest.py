import os
import sys
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from diffusers import (
    ControlNetModel,
    AutoencoderKL,
    UNet2DConditionModel,
    DDPMScheduler,
)
from PIL import Image
from tqdm.auto import tqdm
from nuscenes.nuscenes import NuScenes
import bitsandbytes as bnb 

# ================= ğŸš€ è·¯å¾„é…ç½® (æ ¹æ®ä½ çš„ find ç»“æœä¿®æ­£) =================

# 1. åŸå§‹ nuScenes æ•°æ® (v1.0-mini, samples) åœ¨è¿™é‡Œ
NUSC_ROOT = "./nuScenes"

# 2. ç”Ÿæˆçš„åœ°å›¾ (hdmaps) åœ¨è¿™é‡Œ
# æ ¹æ®ä½ çš„åé¦ˆ: ./training_data/hdmaps
DATA_ROOT = "./training_data"

OUTPUT_DIR = "./models/unimvm_5090_paper"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# 3. æ€§èƒ½å‚æ•° (RTX 5090 æ»¡è¡€ç‰ˆ)
HEIGHT = 256
WIDTH = 448
BATCH_SIZE = 8           # 5090 æ˜¾å­˜å¤§ï¼Œç›´æ¥æ‹‰æ»¡
MAX_TRAIN_STEPS = 5000   # çº¦1å°æ—¶
LEARNING_RATE = 1e-5

# 4. ç½‘ç»œåŠ é€Ÿ
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
MODEL_NAME = "runwayml/stable-diffusion-v1-5" 

# =================================================================

class UniMVMDataset(Dataset):
    def __init__(self, processed_root, nusc_root):
        # è¿™é‡Œä¼šæ‹¼æ¥æˆ ./training_data/hdmaps
        self.map_dir = os.path.join(processed_root, "hdmaps")
        print(f"ğŸš€ [1/3] æ­£åœ¨åŠ è½½ nuScenes... (Root: {nusc_root})")
        
        # --- è·¯å¾„é˜²å¾¡æ€§æ£€æŸ¥ ---
        if not os.path.exists(nusc_root):
            print(f"âŒ ä¸¥é‡é”™è¯¯: æ‰¾ä¸åˆ° nuScenes æ–‡ä»¶å¤¹: {nusc_root}")
            sys.exit(1)
            
        version_path = os.path.join(nusc_root, "v1.0-mini")
        if not os.path.exists(version_path):
            print(f"âŒ ä¸¥é‡é”™è¯¯: åœ¨ {nusc_root} é‡Œæ²¡æ‰¾åˆ° v1.0-miniï¼")
            sys.exit(1)

        if not os.path.exists(self.map_dir):
             print(f"âŒ ä¸¥é‡é”™è¯¯: æ‰¾ä¸åˆ°åœ°å›¾æ–‡ä»¶å¤¹: {self.map_dir}")
             print("è¯·ç¡®è®¤ä½ æ˜¯å¦æœ‰ ./training_data/hdmaps è¿™ä¸ªç›®å½•")
             sys.exit(1)
        # ---------------------

        try:
            self.nusc = NuScenes(version='v1.0-mini', dataroot=nusc_root, verbose=False)
        except Exception as e:
            print(f"âŒ nuScenes åˆå§‹åŒ–æŠ¥é”™: {e}")
            sys.exit(1)

        self.data_pairs = []
        map_files = [f for f in os.listdir(self.map_dir) if f.endswith('.png')]
        
        print(f"ğŸ” [2/3] æ­£åœ¨åŒ¹é…æ•°æ® (æ‰¾åˆ° {len(map_files)} å¼ åœ°å›¾)...")
        # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šåœ°å›¾æ–‡ä»¶å = sample_token.png
        # æˆ‘ä»¬è¦é€šè¿‡ sample_token æ‰¾åˆ°å¯¹åº”çš„çœŸå®ç…§ç‰‡
        for f in tqdm(map_files):
            sample_token = f.split('.')[0]
            try:
                sample = self.nusc.get('sample', sample_token)
                cam_token = sample['data']['CAM_FRONT']
                cam_data = self.nusc.get('sample_data', cam_token)
                
                # çœŸå®ç…§ç‰‡è·¯å¾„
                cam_path = os.path.join(nusc_root, cam_data['filename'])
                # åœ°å›¾è·¯å¾„
                map_path = os.path.join(self.map_dir, f)
                
                if os.path.exists(cam_path) and os.path.exists(map_path):
                    self.data_pairs.append((map_path, cam_path))
            except:
                continue
                
        print(f"âœ… [3/3] æ•°æ®åŒ¹é…å®Œæ¯•! æœ‰æ•ˆè®­ç»ƒæ ·æœ¬: {len(self.data_pairs)} å¯¹")
        
        if len(self.data_pairs) == 0:
            print("âŒ è­¦å‘Š: åŒ¹é…æ•°é‡ä¸º 0ï¼")
            print("å¯èƒ½åŸå› ï¼š")
            print("1. training_data é‡Œçš„åœ°å›¾æ–‡ä»¶åä¸å¯¹ï¼ˆä¸æ˜¯ token.pngï¼‰")
            print("2. nuScenes/samples æ–‡ä»¶å¤¹é‡Œæ²¡æœ‰å¯¹åº”çš„ jpg å›¾ç‰‡")
            sys.exit(1)

        self.transform = transforms.Compose([
            transforms.Resize((HEIGHT, WIDTH)),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5]),
        ])

    def __len__(self):
        return len(self.data_pairs)

    def __getitem__(self, idx):
        map_path, cam_path = self.data_pairs[idx]
        control_image = Image.open(map_path).convert("RGB")
        target_image = Image.open(cam_path).convert("RGB")
        
        return {
            "pixel_values": self.transform(target_image),
            "conditioning_pixel_values": self.transform(control_image)
        }

def train():
    device = torch.device("cuda")
    torch.backends.cuda.matmul.allow_tf32 = True
    
    print(f"ğŸ”¥ 5090 å¼•æ“å¯åŠ¨ | Res: {HEIGHT}x{WIDTH} | Steps: {MAX_TRAIN_STEPS}")
    
    # åŠ è½½æ¨¡å‹ (ä¼šè‡ªåŠ¨æ–­ç‚¹ç»­ä¼ ä¸‹è½½)
    noise_scheduler = DDPMScheduler.from_pretrained(MODEL_NAME, subfolder="scheduler")
    vae = AutoencoderKL.from_pretrained(MODEL_NAME, subfolder="vae").to(device)
    unet = UNet2DConditionModel.from_pretrained(MODEL_NAME, subfolder="unet").to(device)
    controlnet = ControlNetModel.from_unet(unet).to(device)
    
    vae.requires_grad_(False)
    unet.requires_grad_(False)
    controlnet.train()
    
    # ä¼˜åŒ–å™¨
    try:
        import bitsandbytes as bnb
        optimizer = bnb.optim.AdamW8bit(controlnet.parameters(), lr=LEARNING_RATE)
        print("âœ¨ å¯ç”¨ bitsandbytes 8-bit ä¼˜åŒ–")
    except ImportError:
        optimizer = torch.optim.AdamW(controlnet.parameters(), lr=LEARNING_RATE)
    
    # æ•°æ®åŠ è½½
    dataset = UniMVMDataset(DATA_ROOT, NUSC_ROOT)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    
    # è®­ç»ƒå¾ªç¯
    progress_bar = tqdm(range(MAX_TRAIN_STEPS), desc="Training")
    global_step = 0
    
    while global_step < MAX_TRAIN_STEPS:
        for batch in dataloader:
            with torch.cuda.amp.autocast():
                latents = vae.encode(batch["pixel_values"].to(device)).latent_dist.sample()
                latents = latents * vae.config.scaling_factor
                
                control_image = batch["conditioning_pixel_values"].to(device)
                noise = torch.randn_like(latents)
                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device)
                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
                
                empty_text_embeds = torch.zeros((latents.shape[0], 77, 768), device=device)
                
                down_res, mid_res = controlnet(
                    noisy_latents,
                    timesteps,
                    encoder_hidden_states=empty_text_embeds,
                    controlnet_cond=control_image,
                    return_dict=False,
                )
                
                model_pred = unet(
                    noisy_latents,
                    timesteps,
                    encoder_hidden_states=empty_text_embeds,
                    down_block_additional_residuals=down_res,
                    mid_block_additional_residual=mid_res,
                ).sample
                
                loss = F.mse_loss(model_pred.float(), noise.float(), reduction="mean")
            
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            
            progress_bar.update(1)
            progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})
            global_step += 1
            
            if global_step >= MAX_TRAIN_STEPS:
                break

    controlnet.save_pretrained(OUTPUT_DIR)
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {OUTPUT_DIR}")

if __name__ == "__main__":
    train()