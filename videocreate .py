import os
import torch
import numpy as np
import matplotlib.pyplot as plt
import gradio as gr
import imageio
from PIL import Image
from io import BytesIO
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from openai import OpenAI

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================

# âš ï¸ å®‰å…¨è­¦å‘Šï¼šè¿™æ˜¯ä½ çš„ç§é’¥ï¼Œè¯·å‹¿ä¸Šä¼ æ­¤æ–‡ä»¶åˆ° GitHubï¼
# å¦‚æœä¸Šä¼ ä»£ç ï¼Œè¯·åŠ¡å¿…å°†æ­¤å¤„æ”¹å› os.getenv("LLM_API_KEY")
API_KEY = "åœ¨è¿™é‡Œå¡«å…¥ä½ çš„DeepSeek_Key" 

BASE_URL = "https://api.deepseek.com"
MODEL_PATH = "/root/autodl-tmp/models/unimvm_video_model"
BASE_MODEL = "runwayml/stable-diffusion-v1-5"

# --- ğŸï¸ è§†é¢‘æµç•…åº¦æ ¸å¿ƒå‚æ•° ---
TARGET_FPS = 12        # ç›®æ ‡å¸§ç‡ï¼š12 FPS (äººçœ¼æµç•…æ ‡å‡†)
VIDEO_DURATION = 10    # è§†é¢‘æ—¶é•¿ï¼š10ç§’
TOTAL_FRAMES = TARGET_FPS * VIDEO_DURATION # æ€»å¸§æ•°ï¼š120å¸§
# ==============================================

pipe = None

def load_model():
    global pipe
    if pipe is None:
        print("â³ æ­£åœ¨åŠ è½½æ¨¡å‹...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        try:
            controlnet = ControlNetModel.from_pretrained(MODEL_PATH, torch_dtype=torch.float16)
            pipe = StableDiffusionControlNetPipeline.from_pretrained(
                BASE_MODEL, controlnet=controlnet, torch_dtype=torch.float16
            )
            pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
            pipe.to(device)
            # å¼€å¯æ˜¾å­˜ä¼˜åŒ–
            pipe.enable_model_cpu_offload()
            print("âœ… æ¨¡å‹åŠ è½½å®Œæ¯•ï¼")
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹åŠ è½½è­¦å‘Š: {e}")
            print(f"è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨: {MODEL_PATH}")
    return pipe

def get_trajectory_from_llm(prompt):
    """ä½¿ç”¨ DeepSeek è·å–åŸå§‹è½¨è¿¹"""
    if not API_KEY or "ä½ çš„" in API_KEY:
        print("âŒ é”™è¯¯ï¼šè¯·å…ˆåœ¨ä»£ç ç¬¬ 16 è¡Œå¡«å…¥æ­£ç¡®çš„ API Keyï¼")
        return np.linspace(0, 0, 30)

    client = OpenAI(api_key=API_KEY, base_url=BASE_URL)
    try:
        print(f"ğŸ¤– DeepSeek æ­£åœ¨è§„åˆ’è·¯å¾„...")
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªè‡ªåŠ¨é©¾é©¶è§„åˆ’å¸ˆã€‚è¯·è¾“å‡ºæœªæ¥10ç§’çš„30ä¸ªæ¨ªå‘åæ ‡ç‚¹ï¼Œé€—å·åˆ†éš”ã€‚èŒƒå›´-4åˆ°4ï¼ˆè´Ÿæ•°ä¸ºå·¦ï¼Œæ­£æ•°ä¸ºå³ï¼‰ã€‚ä»…è¾“å‡ºæ•°å­—ã€‚"},
                {"role": "user", "content": f"æŒ‡ä»¤: {prompt}"},
            ],
            temperature=0.1
        )
        content = response.choices[0].message.content.strip()
        # æ•°æ®æ¸…æ´—
        content = content.replace('\n', ',').replace(' ', '')
        traj = np.fromstring(content, sep=',')
        
        # å…œåº•è¡¥å…¨
        if len(traj) < 30:
            traj = np.pad(traj, (0, 30 - len(traj)), 'edge')
        return traj[:30]
    except Exception as e:
        print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
        # å¤±è´¥æ—¶è¿”å›ç›´çº¿
        return np.linspace(0, 0, 30)

def interpolate_trajectory(original_traj, target_length):
    """
    ğŸ§® æ’å€¼ç®—æ³•ï¼šå°† 30 ä¸ªç‚¹å¹³æ»‘æ‰©å±•åˆ° 120 ä¸ªç‚¹
    """
    old_indices = np.linspace(0, 10, len(original_traj))
    new_indices = np.linspace(0, 10, target_length)
    new_traj = np.interp(new_indices, old_indices, original_traj)
    return new_traj

def draw_smooth_map(trajectory, frame_idx, window_size=40):
    """
    ç»˜å›¾å‡½æ•° (é€‚é… 120 å¸§çš„å¤§çª—å£)
    """
    plt.figure(figsize=(4, 2.5), dpi=100)
    plt.style.use('dark_background')
    
    # é˜²æ­¢æ•°ç»„è¶Šç•Œ
    padded_traj = np.pad(trajectory, (0, window_size), 'edge')
    
    start_y = frame_idx
    end_y = frame_idx + window_size
    
    # ç»˜åˆ¶è½¦é“çº¿
    y_bg = np.arange(window_size)
    plt.plot(np.zeros_like(y_bg) - 2.0, y_bg, color='white', linestyle='--', alpha=0.3)
    plt.plot(np.zeros_like(y_bg) + 2.0, y_bg, color='white', linestyle='--', alpha=0.3)
    
    # ç»˜åˆ¶çº¢è‰²è½¨è¿¹
    current_traj_segment = padded_traj[start_y:end_y]
    plt.plot(current_traj_segment, np.arange(len(current_traj_segment)), color='red', linewidth=4)
    
    plt.xlim(-5, 5); plt.ylim(0, window_size); plt.axis('off'); plt.tight_layout(pad=0)
    
    buf = BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)
    buf.seek(0)
    plt.close()
    return Image.open(buf).convert("RGB").resize((448, 256))

def generate_smooth_video(user_prompt):
    pipeline = load_model()
    if pipeline is None:
        return None, "æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„"

    # 1. è·å– LLM è§„åˆ’
    raw_traj = get_trajectory_from_llm(user_prompt)
    
    # 2. æ’å€¼å˜å¹³æ»‘ (30 -> 120)
    smooth_traj = interpolate_trajectory(raw_traj, TOTAL_FRAMES)
    
    print(f"ğŸ¬ å¼€å§‹æ¸²æŸ“... (FPS: {TARGET_FPS} | æ€»å¸§æ•°: {TOTAL_FRAMES})")
    
    frames = []
    generator = torch.Generator(device="cuda").manual_seed(42)
    # åŠ¨æ€è°ƒæ•´è§†é‡ï¼šæ€»æ˜¯çœ‹æœªæ¥çº¦ 3 ç§’çš„è·¯
    window_size = int(TOTAL_FRAMES / 3) 

    for i in range(TOTAL_FRAMES):
        if i % 10 == 0:
            print(f"ğŸš€ è¿›åº¦: {i}/{TOTAL_FRAMES} å¸§")
            
        map_img = draw_smooth_map(smooth_traj, i, window_size=window_size)
        
        # 3. ç”Ÿæˆæ¯ä¸€å¸§
        frame = pipeline(
            prompt=f"first person view driving video, {user_prompt}, realistic highway, 4k, motion blur",
            negative_prompt="blurry, distorted, text, low quality, cartoon",
            image=map_img,
            num_inference_steps=15, # æ­¥æ•°è°ƒä½è‡³15ä»¥åŠ å¿«é€Ÿåº¦
            generator=generator
        ).images[0]
        
        frames.append(np.array(frame))

    video_path = "smooth_driving_12fps.mp4"
    imageio.mimsave(video_path, frames, fps=TARGET_FPS)
    print(f"ğŸ‰ è§†é¢‘ç”Ÿæˆå®Œæˆ: {video_path}")
    
    return draw_smooth_map(smooth_traj, 0, window_size), video_path

# ================= ğŸ¨ ç•Œé¢å¯åŠ¨ =================
with gr.Blocks(title="UniMVM Pro (Port 6008)") as demo:
    gr.Markdown(f"# ğŸš— UniMVM è‡ªåŠ¨é©¾é©¶è§†é¢‘ç”Ÿæˆ (Proç‰ˆ)")
    gr.Markdown(f"**çŠ¶æ€**: ç«¯å£ 6008 | {TARGET_FPS} FPS | 10ç§’æ—¶é•¿")
    
    with gr.Row():
        txt_input = gr.Textbox(label="è¾“å…¥æŒ‡ä»¤", value="å‘å·¦å¹³ç¨³å˜é“", placeholder="ä¾‹å¦‚ï¼šå‘å³æ€¥è½¬å¼¯")
        btn_submit = gr.Button("ğŸ¬ å¼€å§‹æ¸²æŸ“", variant="primary")
        
    with gr.Row():
        img_pre = gr.Image(label="è½¨è¿¹é¢„è§ˆ", type="pil")
        vid_out = gr.Video(label="æœ€ç»ˆè§†é¢‘")
        
    btn_submit.click(fn=generate_smooth_video, inputs=txt_input, outputs=[img_pre, vid_out])

if __name__ == "__main__":
    # ã€å…³é”®ä¿®æ”¹ã€‘è¿™é‡Œæ”¹æˆäº† 6008 ç«¯å£ï¼Œé¿å¼€å†²çª
    try:
        demo.queue().launch(server_name="0.0.0.0", server_port=6008)
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        print("ğŸ’¡ å»ºè®®ï¼šå°è¯•ä¿®æ”¹ä»£ç æœ€åä¸€è¡Œï¼Œæ¢æˆ server_port=6009 è¯•è¯•")
