import os
import torch
import numpy as np
import matplotlib.pyplot as plt
import gradio as gr
import imageio
from PIL import Image
from io import BytesIO
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from openai import OpenAI

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
import os
# ä¼˜å…ˆè¯»å–ç¯å¢ƒå˜é‡ï¼Œè¯»ä¸åˆ°å°±ç”¨ç©ºå­—ç¬¦ä¸² (è®©ç”¨æˆ·è‡ªå·±å¡«)
API_KEY = os.getenv("LLM_API_KEY", "") 
# æˆ–è€…ç›´æ¥ç•™ç©ºï¼Œå†™ä¸ªæ³¨é‡Šæé†’ç”¨æˆ·å¡«
# API_KEY = "å¡«å…¥ä½ çš„DeepSeek_Key"
BASE_URL = "https://api.deepseek.com"

MODEL_PATH = "/root/autodl-tmp/models/unimvm_video_model"
BASE_MODEL = "runwayml/stable-diffusion-v1-5"
# ==============================================

pipe = None

def load_model():
    global pipe
    if pipe is None:
        print("â³ æ­£åœ¨åŠ è½½æ¨¡å‹...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        try:
            controlnet = ControlNetModel.from_pretrained(MODEL_PATH, torch_dtype=torch.float16)
            pipe = StableDiffusionControlNetPipeline.from_pretrained(
                BASE_MODEL, controlnet=controlnet, torch_dtype=torch.float16
            )
            pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
            pipe.to(device)
            # ä½¿ç”¨æ›´é€šç”¨çš„æ˜¾å­˜ä¼˜åŒ–ï¼Œé¿å… xformers æŠ¥é”™
            pipe.enable_model_cpu_offload()
            print("âœ… æ¨¡å‹åŠ è½½å®Œæ¯•ï¼")
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹åŠ è½½è­¦å‘Š: {e}")
    return pipe

def get_trajectory_30pts(prompt):
    client = OpenAI(api_key=API_KEY, base_url=BASE_URL)
    try:
        print(f"ğŸ¤– LLM æ­£åœ¨è§„åˆ’è·¯å¾„...")
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªè‡ªåŠ¨é©¾é©¶è§„åˆ’å¸ˆã€‚è¯·è¾“å‡ºæœªæ¥10ç§’çš„30ä¸ªæ¨ªå‘åæ ‡ç‚¹ï¼Œé€—å·åˆ†éš”ã€‚èŒƒå›´-4åˆ°4ã€‚"},
                {"role": "user", "content": f"æŒ‡ä»¤: {prompt}"},
            ],
            temperature=0.1
        )
        content = response.choices[0].message.content.strip()
        traj = np.fromstring(content, sep=',')
        # è¡¥é½åˆ° 30 ä¸ªç‚¹
        if len(traj) < 30:
            traj = np.pad(traj, (0, 30 - len(traj)), 'edge')
        return traj[:30]
    except Exception as e:
        print(f"LLM æ•…éšœ: {e}")
        return np.linspace(0, 0, 30)

def draw_slow_map(trajectory, frame_idx, total_frames=30):
    """ä¿®å¤è¶Šç•Œé—®é¢˜çš„ç»˜å›¾å‡½æ•°"""
    plt.figure(figsize=(4, 2.5), dpi=100)
    plt.style.use('dark_background')
    
    window_size = 10
    
    # ã€ä¿®å¤é‡ç‚¹ã€‘ï¼šå¯¹è½¨è¿¹è¿›è¡Œæœ«ç«¯å¡«å……ï¼Œé˜²æ­¢åˆ‡ç‰‡è¶Šç•Œ
    # è¿™æ ·å½“ frame_idx å¢åŠ æ—¶ï¼Œåé¢æ€»æ˜¯æœ‰æ•°æ®å¯ä»¥ç”»
    padded_traj = np.pad(trajectory, (0, window_size), 'edge')
    
    start_y = frame_idx
    end_y = frame_idx + window_size
    
    # ç»˜åˆ¶èƒŒæ™¯è½¦é“çº¿
    y_bg = np.arange(window_size)
    plt.plot(np.zeros_like(y_bg) - 2.0, y_bg, color='white', linestyle='--', alpha=0.3)
    plt.plot(np.zeros_like(y_bg) + 2.0, y_bg, color='white', linestyle='--', alpha=0.3)
    
    # ç»˜åˆ¶å½“å‰çª—å£å†…çš„è½¨è¿¹
    current_traj_segment = padded_traj[start_y:end_y]
    plt.plot(current_traj_segment, np.arange(len(current_traj_segment)), color='red', linewidth=4)
    
    plt.xlim(-5, 5); plt.ylim(0, 10); plt.axis('off'); plt.tight_layout(pad=0)
    
    buf = BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)
    buf.seek(0)
    plt.close()
    return Image.open(buf).convert("RGB").resize((448, 256))

def generate_slow_video(user_prompt):
    traj = get_trajectory_30pts(user_prompt)
    pipeline = load_model()
    
    fps = 3 
    num_frames = 30 
    frames = []
    
    print(f"ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘...")
    generator = torch.Generator(device="cuda").manual_seed(42)

    for i in range(num_frames):
        print(f"æ¸²æŸ“ä¸­: {i+1}/{num_frames}")
        map_img = draw_slow_map(traj, i, num_frames)
        
        # æ¯ä¸€å¸§çš„ç”Ÿæˆ
        frame = pipeline(
            prompt=f"first person view driving video, {user_prompt}, realistic highway, 4k",
            negative_prompt="blurry, distorted, text",
            image=map_img,
            num_inference_steps=20,
            generator=generator
        ).images[0]
        
        frames.append(np.array(frame))

    video_path = "stable_driving_3fps.mp4"
    imageio.mimsave(video_path, frames, fps=fps)
    return draw_slow_map(traj, 0, num_frames), video_path

# ================= ğŸ¨ UI =================
with gr.Blocks() as demo:
    gr.Markdown("# ğŸš— UniMVM è‡ªåŠ¨é©¾é©¶è§†é¢‘ç”Ÿæˆ (ä¿®å¤ç‰ˆ)")
    with gr.Row():
        txt_input = gr.Textbox(label="è¾“å…¥æŒ‡ä»¤", value="å‘å·¦å¹³ç¨³å˜é“")
        btn_submit = gr.Button("ğŸš€ æ¸²æŸ“è§†é¢‘", variant="primary")
    with gr.Row():
        img_pre = gr.Image(label="åˆå§‹è½¨è¿¹é¢„è§ˆ")
        vid_out = gr.Video(label="ç”Ÿæˆç»“æœ")
    btn_submit.click(fn=generate_slow_video, inputs=txt_input, outputs=[img_pre, vid_out])

if __name__ == "__main__":
    demo.queue().launch(server_name="0.0.0.0", server_port=6006)